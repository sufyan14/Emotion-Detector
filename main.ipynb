{"cells":[{"cell_type":"markdown","metadata":{},"source":["/* Run in Google Collab */"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!wget https://www.dropbox.com/s/w3zlhing4dkgeyb/train.zip?dl=0\n","\n","!unzip train.zip?dl=0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from keras.applications.mobilenet import MobileNet, preprocess_input\n","from keras.models import Model\n","from keras.layers import Flatten, Dense\n","\n","from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img"]},{"cell_type":"markdown","metadata":{},"source":["## Model Building"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["base_model = MobileNet(input_shape=(224,224,3), include_top=False)\n","\n","# To prevent model from retraining\n","\n","for layer in base_model.layers:\n","  layer.trainable = False\n","\n","x = Flatten()(base_model.output)\n","x = Dense(units=7, activation='softmax')(x)\n","\n","model = Model(base_model.input, x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Layers of the model\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{},"source":["## Preparing Data Using Data Generator"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_dataG = ImageDataGenerator(\n","    zoom_range = 0.2,\n","    shear_range = 0.2,\n","    horizontal_flip =True,\n","    rescale = 1./255\n",")\n","\n","train_data = train_dataG.flow_from_directory(directory = \"/content/train\",\n","                                             target_size = (224,224),\n","                                             batch_size = 32)\n","\n","# Batch size indicates how many cycles it will run\n","\n","train_data.class_indices"]},{"cell_type":"markdown","metadata":{},"source":["## Validating the Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_dataG = ImageDataGenerator(rescale = 1/255)\n","\n","val_data = val_dataG.flow_from_directory(directory = \"/content/train\",\n","                                         target_size=(224,224),\n","                                         batch_size=32)"]},{"cell_type":"markdown","metadata":{},"source":["## Visualizing the Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check the current backend, should be TkAgg\n","print(\"Matplotlib Backend:\", plt.get_backend())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualizing images\n","\n","t_img, label = train_data.next()\n","\n","# Function for plotting images\n","\n","def plotImages(img_arr, label):\n","\n","  count = 0\n","  for im, l in zip(img_arr, label):\n","    plt.imshow(im)\n","    plt.title(im.shape)\n","    plt.axis = False\n","    plt.show()\n","\n","    count += 1\n","    if count == 10:\n","      break\n","\n","plotImages(t_img, label)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Checkpoints and Early Stopping"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","# For Early Stopping \n","es = EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=5, verbose=1, mode='auto')\n","\n","# For Model Checkpoint\n","mc = ModelCheckpoint(filepath=\"best_model.h5\", monitor='val_accuracy', verbose=1, save_best_only = True, mode='auto')\n","\n","call_back = [es, mc]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hist = model.fit_generator(train_data, steps_per_epoch=10, epochs=30, validation_data = val_data, \n","                           validation_steps=8,callbacks=[es,mc])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loading best fit model\n","\n","from keras.models import load_model\n","model = load_model(\"/content/best_model.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["h = hist.history\n","h.keys()"]},{"cell_type":"markdown","metadata":{},"source":["## Plotting Accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(h['accuracy'])\n","plt.plot(h['val_accuracy'], c = 'red')\n","plt.title(\"accuracy vs validated accuracy\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(h['loss'])\n","plt.plot(h['val_loss'], c = 'red')\n","plt.title(\"loss vs validated loss\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Map op values\n","\n","op = dict(zip(train_data.class_indices.values(), train_data.class_indices.keys()))"]},{"cell_type":"markdown","metadata":{},"source":["## Checking Class of uploaded image"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Input/Upload any image from your desktop\n","\n","path = '/content/image.jpg'\n","img = load_img(path, target_size=(224,224))\n","\n","i = img_to_array(img)/255\n","input_arr = np.array([i])\n","input_arr.shape\n","\n","predict = np.argmax(model.predict(input_arr))\n","\n","print(f\" Image is of {op[predict]}\")\n","\n","# Display the image\n","plt.imshow(input_arr[0])\n","plt.title(\"Uploaded Image\")\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
